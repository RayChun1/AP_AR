{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwU892KrHNd4"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, sum as spark_sum, to_date\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pyspark.sql import SparkSession\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4rLygICHUbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ee288c-62ed-4c5c-d982-56087995d9af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"FinancialETL\").getOrCreate()"
      ],
      "metadata": {
        "id": "xHjersuNYc5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from Azure Data Lake\n",
        "ap_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/accounts-payable/*.csv\", header=True, inferSchema=True)\n",
        "ar_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/accounts-receivable/*.csv\", header=True, inferSchema=True)\n",
        "bank_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/bank-transactions/*.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4GbiKx1nCOi",
        "outputId": "11e3006c-4c19-49b4-bc3f-cb9aa786cf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, age: string, sex: string, cp: string, trestbps: string, chol: string, fbs: string, restecg: string, thalach: string, exang: string, oldpeak: string, slope: string, ca: string, thal: string, target: string]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize date format\n",
        "ap_df = ap_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))\n",
        "ar_df = ar_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))\n",
        "bank_df = bank_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdxycr-3nE84",
        "outputId": "88c2a87f-dcc9-485a-d498-ba84fd205d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=52, sex=1, cp=0, trestbps=125, chol=212, fbs=0, restecg=1, thalach=168, exang=0, oldpeak=1.0, slope=2, ca=2, thal=3, target=0),\n",
              " Row(age=53, sex=1, cp=0, trestbps=140, chol=203, fbs=1, restecg=0, thalach=155, exang=1, oldpeak=3.1, slope=0, ca=0, thal=3, target=0),\n",
              " Row(age=70, sex=1, cp=0, trestbps=145, chol=174, fbs=0, restecg=1, thalach=125, exang=1, oldpeak=2.6, slope=0, ca=0, thal=3, target=0),\n",
              " Row(age=61, sex=1, cp=0, trestbps=148, chol=203, fbs=0, restecg=1, thalach=161, exang=0, oldpeak=0.0, slope=2, ca=1, thal=3, target=0),\n",
              " Row(age=62, sex=0, cp=0, trestbps=138, chol=294, fbs=1, restecg=1, thalach=106, exang=0, oldpeak=1.9, slope=1, ca=3, thal=2, target=0),\n",
              " Row(age=58, sex=0, cp=0, trestbps=100, chol=248, fbs=0, restecg=0, thalach=122, exang=0, oldpeak=1.0, slope=1, ca=0, thal=2, target=1),\n",
              " Row(age=58, sex=1, cp=0, trestbps=114, chol=318, fbs=0, restecg=2, thalach=140, exang=0, oldpeak=4.4, slope=0, ca=3, thal=1, target=0),\n",
              " Row(age=55, sex=1, cp=0, trestbps=160, chol=289, fbs=0, restecg=0, thalach=145, exang=1, oldpeak=0.8, slope=1, ca=1, thal=3, target=0),\n",
              " Row(age=46, sex=1, cp=0, trestbps=120, chol=249, fbs=0, restecg=0, thalach=144, exang=0, oldpeak=0.8, slope=2, ca=0, thal=3, target=0),\n",
              " Row(age=54, sex=1, cp=0, trestbps=122, chol=286, fbs=0, restecg=0, thalach=116, exang=1, oldpeak=3.2, slope=1, ca=2, thal=2, target=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3nEMol9ZQJJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join AP and AR with bank transactions for reconciliation\n",
        "reconciled_df = ap_df.join(bank_df, (ap_df.Amount == bank_df.Amount) & (ap_df.TransactionDate == bank_df.TransactionDate), \"left\")     .withColumn(\"Reconciled\", when(col(\"bank.TransactionID\").isNotNull(), \"Yes\").otherwise(\"No\"))\n"
      ],
      "metadata": {
        "id": "l8jh4Em9P9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate outstanding and overdue transactions\n",
        "outstanding_df = ar_df.groupBy(\"CustomerID\").agg(spark_sum(\"Amount\").alias(\"OutstandingReceivable\"))\n",
        "overdue_df = ap_df.filter(col(\"DueDate\") < col(\"TransactionDate\")).groupBy(\"VendorID\").agg(spark_sum(\"Amount\").alias(\"OverduePayable\"))\n",
        "\n",
        "# Save transformed data to Azure Data Lake\n",
        "reconciled_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/reconciled_transactions/\")\n",
        "outstanding_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/outstanding_receivables/\")\n",
        "overdue_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/overdue_payables/\")\n"
      ],
      "metadata": {
        "id": "l79bVNNJQLTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}