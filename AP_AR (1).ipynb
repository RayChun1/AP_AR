{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwU892KrHNd4"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, sum as spark_sum, to_date\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pyspark.sql import SparkSession\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4rLygICHUbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ee288c-62ed-4c5c-d982-56087995d9af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"FinancialETL\").getOrCreate()"
      ],
      "metadata": {
        "id": "xHjersuNYc5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from Azure Data Lake\n",
        "ap_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/accounts-payable/*.csv\", header=True, inferSchema=True)\n",
        "ar_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/accounts-receivable/*.csv\", header=True, inferSchema=True)\n",
        "bank_df = spark.read.csv(\"abfss://datalake@myaccount.dfs.core.windows.net/bank-transactions/*.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4GbiKx1nCOi",
        "outputId": "11e3006c-4c19-49b4-bc3f-cb9aa786cf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, age: string, sex: string, cp: string, trestbps: string, chol: string, fbs: string, restecg: string, thalach: string, exang: string, oldpeak: string, slope: string, ca: string, thal: string, target: string]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize date format\n",
        "ap_df = ap_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))\n",
        "ar_df = ar_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))\n",
        "bank_df = bank_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\"), \"yyyy-MM-dd\"))"
      ],
      "metadata": {
        "id": "Qdxycr-3nE84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3nEMol9ZQJJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join AP and AR with bank transactions for reconciliation\n",
        "reconciled_df = ap_df.join(bank_df, (ap_df.Amount == bank_df.Amount) & (ap_df.TransactionDate == bank_df.TransactionDate), \"left\")     .withColumn(\"Reconciled\", when(col(\"bank.TransactionID\").isNotNull(), \"Yes\").otherwise(\"No\"))\n"
      ],
      "metadata": {
        "id": "l8jh4Em9P9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate outstanding and overdue transactions\n",
        "outstanding_df = ar_df.groupBy(\"CustomerID\").agg(spark_sum(\"Amount\").alias(\"OutstandingReceivable\"))\n",
        "overdue_df = ap_df.filter(col(\"DueDate\") < col(\"TransactionDate\")).groupBy(\"VendorID\").agg(spark_sum(\"Amount\").alias(\"OverduePayable\"))\n",
        "\n",
        "# Save transformed data to Azure Data Lake\n",
        "reconciled_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/reconciled_transactions/\")\n",
        "outstanding_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/outstanding_receivables/\")\n",
        "overdue_df.write.parquet(\"abfss://datalake@myaccount.dfs.core.windows.net/processed_data/overdue_payables/\")\n"
      ],
      "metadata": {
        "id": "l79bVNNJQLTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}